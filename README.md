# CS885
##演算法結果討論與分析

這次的實驗結果清楚地展示了價值迭代（Value Iteration）、策略迭代（Policy Iteration）以及修改版策略迭代（Modified Policy Iteration）在解決馬可夫決策過程（MDP）問題時的特性與差異。

首先，比較標準的價值迭代與策略迭代。價值迭代需要 26 次迭代才能收斂，而策略迭代僅需 6 次。這個顯著的差異源於兩種演算法的核心機制。策略迭代的每次迭代都包含一個「完整」的策略評估步驟（透過解線性方程組得到當前策略下最精確的價值函數），這使得它能更快地找到最佳策略，因此外部迭代次數較少。然而，解線性方程組的計算成本非常高，特別是在狀態空間很大時。相對地，價值迭代的每次迭代計算非常快（只是一個簡單的 Bellman 更新），但它需要更多次的迭代來讓價值函數逐漸收斂到最佳值。最終，兩者找到了相同的最佳策略。

修改版策略迭代的結果完美地橋接了這兩者之間的關係。

與價值迭代的關係： 當部分策略評估的迭代次數（nIterPPE）設為 1 時，修改版策略迭代的總收斂次數為 26 次，這與價值迭代的結果完全相同。這是因為，在這種設定下，演算法的行為等同於價值迭代：每進行一次 Bellman 更新（部分評估），就馬上進行一次策略改進（取 argmax）。

效能的權衡： 隨著 nIterPPE 的增加（從 2 到 7），總收斂迭代次數顯著下降。這說明在每次策略改進之前，花費更多的計算來獲得一個更準確的價值函數估計，可以讓策略的「品質」提升得更快，從而減少了達到最佳策略所需的總迭代輪數。

與策略迭代的關係： 當 nIterPPE 增加到 7 次以後，總迭代次數穩定在 7 或 8 次左右。這個數字非常接近策略迭代所需的 6 次。這表明，在這個特定的迷宮問題中，進行大約 7 次的 Bellman 更新，就足以讓價值函數的估計非常接近「完整」評估的結果。再增加評估次數（如 8, 9, 10）對策略改進的幫助已經很小，因此總迭代次數不再顯著下降。

結論： 總結來說，這些結果驗證了演算法的理論。價值迭代和策略迭代是求解 MDP 的兩個極端方法。修改版策略迭代則提供了一個彈性的中間方案，允許我們透過調整 nIterPPE 參數，在「單次迭代的計算成本」與「收斂所需的總迭代次數」之間做出權衡。在實際應用中，選擇一個合適的 nIterPPE 可以找到計算效率上的最佳點，使其比單純的價值迭代或策略迭代更快地解決問題。
---
