# CS885

## Part 1: MDP 演算法比較 - 結果討論與分析

本次實驗結果清楚地展示了價值迭代（Value Iteration）、策略迭代（Policy Iteration）以及修改版策略迭代（Modified Policy Iteration）在解決馬可夫決策過程（MDP）問題時的特性與差異。

首先，比較標準的價值迭代與策略迭代。**價值迭代 (Value Iteration)** 需要 **26 次**迭代才能收斂，而**策略迭代 (Policy Iteration)** 僅需 **6 次**。這個顯著的差異源於兩種演算法的核心機制。策略迭代的每次迭代都包含一個「完整」的策略評估步驟（透過解線性方程組得到當前策略下最精確的價值函數），這使得它能更快地找到最佳策略，因此外部迭代次數較少。然而，解線性方程組的計算成本非常高，特別是在狀態空間很大時。相對地，價值迭代的每次迭代計算非常快（只是一個簡單的 Bellman 更新），但它需要更多次的迭代來讓價值函數逐漸收斂到最佳值。最終，兩者找到了相同的最佳策略。

### 修改版策略迭代 (Modified Policy Iteration)

1.  **與價值迭代的關係：**
    當部分策略評估的迭代次數 (`nIterPPE`) 設為 `1` 時，修改版策略迭代的總收斂次數為 **26 次**，這與價值迭代的結果**完全相同**。這是因為，在這種設定下，演算法的行為等同於價值迭代：每進行一次 Bellman 更新（部分評估），就馬上進行一次策略改進（取 argmax）。

2.  **效能的權衡：**
    隨著 `nIterPPE` 的增加（從 `2` 到 `7`），總收斂迭代次數顯著下降。這說明在每次策略改進之前，花費更多的計算來獲得一個更準確的價值函數估計，可以讓策略的「品質」提升得更快，從而減少了達到最佳策略所需的總迭代輪數。

3.  **與策略迭代的關係：**
    當 `nIterPPE` 增加到 `7` 次以後，總迭代次數穩定在 7 或 8 次左右。這個數字非常接近策略迭代所需的 6 次。這表明，在這個特定的迷宮問題中，進行大約 7 次的 Bellman 更新，就足以讓價值函數的估計非常接近「完整」評估的結果。再增加評估次數（如 8, 9, 10）對策略改進的幫助已經很小，因此總迭代次數不再顯著下降。

### Part 1 結論

總結來說，這些結果驗證了演算法的理論。價值迭代和策略迭代是求解 MDP 的兩個極端方法。修改版策略迭代則提供了一個彈性的中間方案，允許我們透過調整 `nIterPPE` 參數，在「單次迭代的計算成本」與「收斂所需的總迭代次數」之間做出權衡。在實際應用中，選擇一個合適的 `nIterPPE` 可以找到計算效率上的最佳點，使其比單純的價值迭代或策略迭代更快地解決問題。

---

## Part 2: Q-Learning 探索率分析 - 結果討論與分析

本實驗旨在探討 Q-learning 演算法中，不同探索率 (Exploration Probability, `epsilon`) 對於 **Agent** 在迷宮環境中學習效率與最終成果的影響。實驗結果完美體現了**探索 (Exploration) 與利用 (Exploitation) 之間的權衡**。

### 實驗結果圖表

![Q-Learning 實驗結果圖](/figs/Assignment1_part2.png)

### 圖表分析與討論

這張圖表清楚地展示了 `epsilon` 值如何影響 **Agent** 的學習過程。

#### 1. 學習速度 (初期階段, Episodes 0-50)

* **高探索率 (`ε = 0.3, 0.5`) 的初期優勢**: 在訓練初期，`ε = 0.5` (紅色) 和 `ε = 0.3` (綠色) 的曲線爬升速度最快。這說明較高的探索率能讓 **Agent** 更快地發現環境中的關鍵資訊（如高獎勵的目標或高懲罰的陷阱），從而迅速建立對環境的初步認知。

* **低探索率 (`ε = 0.05, 0.1`) 的初期學習較慢**: `ε = 0.05` (藍色) 和 `ε = 0.1` (橘色) 的曲線在初期爬升較為平緩。這是因為它們傾向於「利用」已知的、哪怕不是最佳的路徑，導致探索新路徑的速度變慢。

#### 2. 最終策略表現 (收斂階段, Episodes 50-200)

* **低探索率 (`ε = 0.05, 0.1`) 的最終表現最佳**: 隨著訓練的進行，橘色和藍色曲線最終達到了最高的平均獎勵（穩定在 40 到 55 之間）。這表明，一旦 **Agent** 找到了接近最佳的策略，較低的探索率能讓它**最大化地「利用」**這些學到的知識，從而穩定地獲得高分。

* **中等探索率 (`ε = 0.3`) 的表現次之**: 綠色曲線雖然初期學習很快，但最終的平均獎勵卻穩定在一個較低的水平。原因在於，即使學到了好策略，仍有 **30% 的機率會選擇隨機動作**，這種持續的探索行為阻礙了它穩定地執行最佳路徑。

* **高探索率 (`ε = 0.5`) 的表現最差**: 紅色曲線的最終表現最差。**50% 的探索率意味著 Agent 有一半的時間都在「胡亂嘗試」**，而不是執行學到的最佳策略，導致它幾乎無法穩定地獲得高分。

### Part 2 結論

1.  **Epsilon 的選擇是一個關鍵的權衡**：太低的 epsilon 可能導致學習速度慢；太高的 epsilon 則會妨礙 **Agent** 利用已學到的知識，導致最終表現不佳。
2.  **最佳策略**：在本次實驗中，`ε = 0.05` 和 `ε = 0.1` 提供了最佳的平衡，它們的探索性足以找到最佳路徑，利用性也足夠高，能夠在找到路徑後持續獲得高分。
3.  **對 Q-value 和策略的影響**：一個合適的 epsilon 能幫助演算法收斂到更準確的 Q-value，從而導出最佳策略。過高的 epsilon 會因為持續的隨機行為引入太多「噪音」，使得 Q-value 難以穩定收斂。

---

## Part 3: DQN 演算法超參數分析 

本實驗旨在分析深度 Q 網路 (DQN) 中兩個關鍵超參數——**目標網路更新頻率 (Target Network Update Frequency)** 與 **小批量大小 (Minibatch Size)**——對於 Agent 在 `CartPole-v1` 環境中學習效能的影響。
> **Important Note:** 本部分的程式碼是使用 `gymnasium` 函式庫所開發。此函式庫為 OpenAI `gym` 的官方後繼維護版本。請確保您已安裝 `gymnasium` 以順利執行程式碼。

---

## 實驗一：目標網路更新頻率的影響

本實驗旨在探討更新目標網路 (`Q_target`) 的頻率對學習穩定性與最終成效的影響。我們比較了四種更新頻率：每 1、10、50 和 100 個 episode 更新一次。

### 實驗結果圖表

![Impact of Target Network Update Frequency](/figs/Assignment1_part3_target.png)

### 圖表分析與討論

1.  **最佳表現 (Update Freq=10)**:
    綠色曲線 (頻率=10) 明顯是本次實驗的最佳設定。它不僅學習速度最快（在約 125 個 episode 後迅速爬升），也達到了最高且最穩定的平均獎勵（約 250）。這表明，每 10 個 episode 更新一次目標網路，為 Q 網路提供了一個既不過時、也足夠穩定的學習目標。

2.  **更新過於頻繁 (Update Freq=1)**:
    藍色曲線 (頻率=1) 顯示，過於頻繁地更新目標網路會導致學習不穩定。由於目標網路的權重與主 Q 網路的權重幾乎同步變化，學習目標變得不穩定，就像「追逐自己的尾巴」。這使得網路難以收斂，雖然最終仍在學習，但其過程充滿了較大的變異（較寬的陰影區域），且整體表現不如頻率為 10 的設定。

3.  **更新過於緩慢 (Update Freq=50, 100)**:
    紅色 (頻率=50) 和紫色 (頻率=100) 的曲線表現最差。當更新頻率太低時，目標網路提供的 Q 值會變得「過時」(stale)。主 Q 網路的策略已經改善，但它學習的目標卻仍然基於一個很久以前的、較差的策略，這嚴重拖慢了學習進度。Agent 需要等待很長時間，目標網路才能「趕上」主網路的進步。

### 與價值迭代 (Value Iteration) 的關聯

目標網路的概念與**價值迭代 (Value Iteration)** 的核心思想有著深刻的聯繫。

在價值迭代中，我們使用第 `k` 次迭代的價值函數 `V_k` 來計算第 `k+1` 次的價值函數 `V_{k+1}`。其更新公式為：
`V_{k+1}(s) <-- max_a [R(s,a) + γ * Σ P(s'|s,a) * V_k(s')]`

在這個過程中，`V_k` 在整個更新 `V_{k+1}` 的過程中是**固定不變**的。

DQN 的更新目標 `y = R + γ * max_a' Q_target(s', a')` 與此非常相似。**目標網路 (`Q_target`) 就扮演了 `V_k` 的角色**。它提供了一個在一段時間內**固定不變的目標**，讓主 Q 網路可以穩定地朝著這個目標進行更新。如果沒有目標網路（或更新頻率為 1），就相當於 `V_k` 和 `V_{k+1}` 同時在變，這會破壞價值迭代的收斂穩定性。因此，一個合適的更新頻率，就像價值迭代中完成一次完整的迭代後再更新價值函數一樣，是確保穩定收斂的關鍵。

---

## 實驗二：Minibatch 大小的影響

本實驗旨在探討從經驗回放緩衝區 (Replay Buffer) 中取樣的 Minibatch 大小對學習效率的影響。我們比較了四種大小：1、10、50 和 100。

### 實驗結果圖表

![Impact of Minibatch Size](/figs/Assignment1_part3_minibatch.png)

### 圖表分析與討論

1.  **最佳表現 (Minibatch Size=50, 100)**:
    紅色 (大小=50) 和紫色 (大小=100) 的曲線表現最佳。它們的學習速度最快，並且達到了最高的平均獎勵。這表明，較大的 Minibatch 能夠提供更穩定、更具代表性的梯度，從而讓網路更新更有效、更平滑。

2.  **表現較差 (Minibatch Size=10)**:
    綠色曲線 (大小=10) 雖然最終也在學習，但其學習速度遠遠慢於較大的 batch size。這意味著雖然 10 個樣本提供了一些穩定性，但梯度的變異性仍然較大，拖慢了學習進度。

3.  **幾乎無法學習 (Minibatch Size=1)**:
    藍色曲線 (大小=1) 的表現極差，Agent 幾乎沒有學到任何有用的策略。這是因為 batch size 為 1 相當於**純粹的隨機梯度下降 (Stochastic Gradient Descent, SGD)**。每一次更新都只基於單一的、充滿隨機性的經驗，導致梯度方向的變異性極大。這種「噪音」極高的更新使得網路權重無法朝著一個一致的方向收斂。

### 與梯度下降 (Gradient Descent) 的關聯

Replay Buffer 和 Minibatch 的使用與梯度下降的變體密切相關。

1.  **隨機梯度下降 (SGD)**: 當 Minibatch Size 為 1 時，我們正是在執行 SGD。每次更新的梯度都基於單一樣本，其優點是計算快，但缺點是梯度變異性極大，導致收斂過程非常不穩定，如藍色曲線所示。

2.  **小批量梯度下降 (Mini-batch GD)**: 當 Minibatch Size 大於 1（如 10, 50, 100）時，我們是在執行 Mini-batch GD。透過對一小批樣本的損失進行平均，我們得到了一個對「真實」梯度（即在整個緩衝區上計算的梯度）的更準確、變異性更低的估計。這在穩定性和計算效率之間取得了很好的平衡。

3.  **批量梯度下降 (Batch GD)**: 如果我們將 Minibatch Size 設定為整個 Replay Buffer 的大小，那就相當於執行了**批量梯度下降**。這會提供最準確的梯度方向，但每次更新的計算成本極高，並且在 RL 中可能因為樣本分佈的劇烈變化而效果不佳。

Replay Buffer 的核心作用是**打破經驗之間的時間相關性**，使得樣本更接近於**獨立同分佈 (I.I.D.)**，這是深度學習模型訓練的基本假設。Minibatch 則是在此基礎上，透過平均多個樣本的梯度來降低更新的隨機性，從而實現了穩定高效的學習。
